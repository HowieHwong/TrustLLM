{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"TrustLLM: Trustworthiness in Large Language Models","text":""},{"location":"index.html#about","title":"About","text":"<p>TrustLLM is a comprehensive study of trustworthiness in large language models (LLMs), including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. The document explains how to use the trustllm python package to help you assess the performance of your LLM in trustworthiness more quickly. For more details about TrustLLM, please refer to this link.</p> <p></p>"},{"location":"index.html#before-evaluation","title":"Before Evaluation","text":""},{"location":"index.html#installation","title":"Installation","text":"<p>Installation via <code>pip</code>:</p> <pre><code>pip install trustllm\n</code></pre> <p>Installation via Github:</p> <pre><code>git clone git@github.com:HowieHwong/TrustLLM.git\n</code></pre> <p>Creat a new environment:</p> <pre><code>conda create --name trustllm python=3.9\n</code></pre> <p>Install required packages:</p> <pre><code>cd trustllm_pkg\npip install .\n</code></pre>"},{"location":"index.html#dataset-download","title":"Dataset Download","text":"<p>Download TrustLLM dataset:</p> <pre><code>from trustllm import dataset_download\n\ndownload_huggingface_dataset(save_path='save_path')\n</code></pre>"},{"location":"index.html#generation","title":"Generation","text":"<p>Note</p> <p>Please note that the LLM you use for evaluation should have a certain level of utility. If its generation/NLP capabilities are weak, it may bias the evaluation results (for example, many evaluation samples may be considered invalid).</p> <p>The datasets are structured in JSON format, where each JSON file consists of a collection of <code>dict()</code>. Within each <code>dict()</code>, there is a key named <code>prompt</code>. Your should utilize the value of <code>prompt</code> key as the input for generation. After generation, you should store the output of LLMs as s new key named <code>res</code> within the same dictionary. Here is an example to generate answer from your LLM:</p> <p>For each dataset, we have configured the temperature setting during model generation. Please refer to this page for the settings.</p> <pre><code>import json\n\nfilename = 'dataset_path.json'\n\n# Load the data from the file\nwith open(filename, 'r') as file:\n    data = json.load(file)\n\n# Process each dictionary and add the 'res' key with the generated output\nfor element in data:\n    element['res'] = generation(element['prompt'])  # Replace 'generation' with your function\n\n# Write the modified data back to the file\nwith open(filename, 'w') as file:\n    json.dump(data, file, indent=4)\n</code></pre>"},{"location":"index.html#start-your-evaluation","title":"Start Your Evaluation","text":"<p>See this page for more details.</p>"},{"location":"index.html#leaderboard","title":"Leaderboard","text":"<p>If you want to view the performance of all models or upload the performance of your LLM, please refer to this link.</p>"},{"location":"index.html#citation","title":"Citation","text":""},{"location":"changelog.html","title":"Changelog","text":""},{"location":"changelog.html#todo-in-coming-versions","title":"\u23f0 TODO in Coming Versions","text":"<ul> <li>Faster and simpler evaluation pipeline</li> <li>Dynamic dataset</li> <li>More fine-grained datasets</li> <li>Chinese output evaluation</li> <li>Downstream application evaluation</li> </ul>"},{"location":"changelog.html#version-010","title":"Version 0.1.0","text":"<p>Release Date: 10th Jan, 2024</p> <p>We have released the first version of the TrustLLM assessment tool, which includes all the evaluation methods from our initial research paper.</p>"},{"location":"faq.html","title":"FAQ","text":""},{"location":"faq.html#language-bias","title":"Language Bias","text":"<p>As we mentioned in our paper, the language output of the model will to some extent affect our evaluation results. For the longformer model, its performance in Chinese is relatively poor. Therefore, when calculating RtA, we consider responses with a Chinese character ratio greater than \\(\\alpha\\) as invalid sample (the default setting of \\(\\alpha\\) is 0.3)</p>"},{"location":"guides/evaluation.html","title":"Start Your Evaluation","text":""},{"location":"guides/evaluation.html#start-your-evaluation","title":"Start Your Evaluation","text":""},{"location":"guides/evaluation.html#api-setting","title":"API Setting","text":"<p>Before starting the evaluation, you need to first set up your OpenAI API (GPT-4-turbo) and Perspective API (used for measuring toxicity).</p> <pre><code>from trustllm import config\n\nconfig.openai_key = 'your-openai-api-key'\n\nconfig.perspective_key = 'your-perspective-api-key'\n</code></pre>"},{"location":"guides/evaluation.html#truthfulness","title":"Truthfulness","text":"<p>Four subsections in truthfulness evaluation:</p> <ul> <li>Misinformation: <code>external.json</code>, <code>internal.json</code></li> <li>Hallucination: <code>hallucination.json</code></li> <li>Sycophancy: <code>sycophancy.json</code></li> <li>Adversarial Factuality: <code>golden_advfactuality.json</code></li> </ul> <p>Requirement:</p> <p></p> <ul> <li>openai api (gpt-4-turbo)</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm import truthfulness\nfrom trustllm import file_process\nfrom trustllm import config\n\nevaluator = truthfulness.TruthfulnessEval()\n</code></pre> <p>Misinformation evaluation:</p> <pre><code>misinformation_internal_data = file_process.load_json('misinformation_internal_data_json_path')\nprint(evaluator.internal_eval(misinformation_internal_data))\n\nmisinformation_external_data = file_process.load_json('misinformation_external_data_json_path')\nprint(evaluator.external_eval(misinformation_external_data))\n</code></pre> <p>Hallucination evaluation:</p> <pre><code>hallucination_data = file_process.load_json('hallucination_data_json_path')\nprint(evaluator.hallucination_eval(hallucination_data))\n</code></pre> <p>Sycophancy evaluation (<code>eval_type</code>: type of evaluation, either <code>persona</code> or <code>preference</code>): <pre><code>sycophancy_data = file_process.load_json('sycophancy_data_json_path')\nprint(evaluator.sycophancy_eval(sycophancy_data, eval_type='persona'))\nprint(evaluator.sycophancy_eval(sycophancy_data, eval_type='preference'))\n</code></pre></p> <p>Adversarial factuality evaluation: <pre><code>adv_fact_data = file_process.load_json('adv_fact_data_json_path')\nprint(evaluator.advfact_eval(adv_fact_data))\n</code></pre></p>"},{"location":"guides/evaluation.html#safety","title":"Safety","text":"<p>Four subsections in safety evaluation:</p> <ul> <li>Jailbreak: <code>jailbreak.json</code></li> <li>Exaggerated Safety: <code>exaggerated_safety.json</code></li> <li>Toxicity: <code>jailbreak.json</code></li> <li>Misuse: <code>misuse.json</code></li> </ul> <p>Requirement:</p> <p> </p> <ul> <li>openai api (gpt-4-turbo)</li> <li>perspective api</li> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm import safety\nfrom trustllm import file_process\nfrom trustllm import config\n\nevaluator = safety.SafetyEval()\n</code></pre> <p>Jailbreak evaluation (<code>eval_type</code>: type of evaluation, either <code>total</code> or <code>single</code>):</p> <pre><code>jailbreak_data = file_process.load_json('jailbreak_data_json_path')\nprint(evaluator.jailbreak_eval(jailbreak_data, eval_type='total')) # return overall RtA\nprint(evaluator.jailbreak_eval(jailbreak_data, eval_type='single')) # return RtA dict for each kind of jailbreak ways\n</code></pre> <p>Exaggerated evaluation:</p> <pre><code>exaggerated_data = file_process.load_json('exaggerated_data_json_path')\nprint(evaluator.exaggerated_eval(exaggerated_data))\n</code></pre> <p>Toxicity evaluation:</p> <p>Note</p> <p>Toxicity data for evaluation is based on the evaluated results of jailbreak data. You should first evaluate jailbreak data and then use evaluated results to conduct toxicity evaluation.</p> <pre><code># If you have not saved evaluated jailbreak results\njailbreak_data = file_process.load_json('jailbreak_data_json_path')\neval_data, _ = evaluator.jailbreak_eval(jailbreak_data, eval_type='total/single', return_data=True)\nprint(evaluator.toxicity_eval(eval_data, perspective_api=config.perspective_key))\n\n# If you have saved evaluated jailbreak results\ntoxicity_data = file_process.load_json('evaluated_jailbreak_results_json_path') # load eval data for toxicity evaluation\nprint(evaluator.toxicity_eval(toxicity_data))\n</code></pre> <p>Misuse evaluation:</p> <pre><code>misuse_data = file_process.load_json('misuse_data_json_path')\nprint(evaluator.misuse_eval(misuse_data))\n</code></pre>"},{"location":"guides/evaluation.html#fairness","title":"Fairness","text":"<p>Three subsections in fairness evaluation:</p> <ul> <li>Stereotype: <code>stereotype_agreement.json</code>, <code>stereotype_recognition.json</code>, <code>stereotype_query_test.json</code></li> <li>Disparagement: <code>disparagement.json</code></li> <li>Preference: <code>preference_force.json</code></li> </ul> <p>Requirement:</p> <p> </p> <ul> <li>openai api (gpt-4-turbo)</li> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm import fairness\nfrom trustllm import file_process\nfrom trustllm import config\n\nevaluator = fairness.FairnessEval()\n</code></pre> <p>Stereotype evaluation:</p> <pre><code>stereotype_recognition_data = file_process.load_json('stereotype_recognition_data_json_path')\nprint(evaluator.stereotype_recognition_eval(stereotype_recognition_data))\n\nstereotype_agreement_data = file_process.load_json('stereotype_agreement_data_json_path')\nprint(evaluator.stereotype_agreement_eval(stereotype_agreement_data))\n\nstereotype_query_test_data = file_process.load_json('stereotype_query_test_data_json_path')\nprint(evaluator.stereotype_query_eval(stereotype_query_test_data))\n</code></pre> <p>Disparagement evaluation: <pre><code>disparagement_data = file_process.load_json('disparagement_data_json_path')\nprint(evaluator.disparagement_eval(disparagement_data))\n</code></pre></p> <p>Preference evaluation: <pre><code>preference_data = file_process.load_json('preference_data_json_path')\nprint(evaluator.preference_eval(preference_data))\n</code></pre></p>"},{"location":"guides/evaluation.html#robustness","title":"Robustness","text":"<p>Two subsections in robustness evaluation:</p> <ul> <li>Natural noise: <code>advglue.json</code>, <code>advinstruction.json</code></li> <li>Out of distribution: <code>ood_generalization.json</code>, <code>ood_detection.json</code></li> </ul> <p>Requirement:</p> <p> </p> <ul> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm import robustness\nfrom trustllm import file_process\nfrom trustllm import config\n\nevaluator = robustness.RobustnessEval()\n</code></pre> <p>Natural noise evaluation:</p> <pre><code>advglue_data = file_process.load_json('advglue_data_json_path')\nprint(evaluator.advglue_eval(advglue_data))\n\nadvinstruction_data = file_process.load_json('advinstruction_data_json_path')\nprint(evaluator.advglue_eval(advinstruction_data))\n</code></pre> <p>OOD evaluation:</p> <pre><code>ood_detection_data = file_process.load_json('ood_detection_data_json_path')\nprint(evaluator.ood_detection(ood_detection_data))\n\nood_generalization_data = file_process.load_json('ood_generalization_data_json_path')\nprint(evaluator.ood_generalization(ood_generalization_data))\n</code></pre>"},{"location":"guides/evaluation.html#privacy","title":"Privacy","text":"<p>Two subsections in privacy evaluation:</p> <ul> <li>Privacy awareness: <code>privacy_awareness_confAIde.json</code>, <code>privacy_awareness_query.json</code></li> <li>Privacy leakage: <code>privacy_leakage.json</code></li> </ul> <p>Requirement:</p> <p> </p> <ul> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm import privacy\nfrom trustllm import file_process\nfrom trustllm import config\n\nevaluator = privacy.PrivacyEval()\n</code></pre> <p>Privacy awareness:</p> <pre><code>privacy_confAIde_data = file_process.load_json('privacy_confAIde_data_json_path')\nprint(evaluator.ConfAIDe_eval(privacy_confAIde_data))\n\nprivacy_awareness_query_data = file_process.load_json('privacy_awareness_query_data_json_path')\nprint(evaluator.awareness_query_eval(privacy_awareness_query_data, type='normal'))\nprint(evaluator.awareness_query_eval(privacy_awareness_query_data, type='aug'))\n</code></pre> <p>Privacy leakage:</p> <pre><code>privacy_leakage_data = file_process.load_json('privacy_leakage_data_json_path')\nprint(evaluator.leakage_eval(privacy_leakage_data))\n</code></pre>"},{"location":"guides/evaluation.html#machine-ethics","title":"Machine Ethics","text":"<p>Three subsections in machine ethics evaluation:</p> <p>Implicit ethics: <code>implicit_ETHICS.json</code>, <code>implicit_SocialChemistry101.json</code> Explicit ethics: <code>explicit_moralchoice.json</code> Emotional awareness: <code>emotional_awareness.json</code></p> <p>Requirement:</p> <p> </p> <ul> <li>openai api (gpt-4-turbo)</li> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm import ethics\nfrom trustllm import file_process\nfrom trustllm import config\n\nevaluator = ethics.EthicsEval()\n</code></pre> <p>Explicit ethics:</p> <p><pre><code>explicit_ethics_data = file_process.load_json('explicit_ethics_data_json_path')\nprint(evaluator.explicit_ethics_eval(explicit_ethics_data, eval_type='low'))\nprint(evaluator.explicit_ethics_eval(explicit_ethics_data, eval_type='high'))\n</code></pre> Implicit ethics:</p> <pre><code>implicit_ethics_data = file_process.load_json('implicit_ethics_data_json_path')\nprint(evaluator.implicit_ethics_eval(implicit_ethics_data, eval_type='ETHICS'))\nprint(evaluator.implicit_ethics_eval(implicit_ethics_data, eval_type='social_norm'))\n</code></pre> <p>Emotional awareness:</p> <pre><code>emotional_awareness_data = file_process.load_json('emotional_awareness_data_json_path')\nprint(evaluator.emotional_awareness_eval(emotional_awareness_data))\n</code></pre>"},{"location":"guides/generation_details.html","title":"Generation Details","text":""},{"location":"guides/generation_details.html#generation-parameters","title":"Generation Parameters","text":"<p>We categorize the tasks in the benchmark into two main groups: Generation and Classification. </p> <p>We employ a temperature setting of 0 for classification tasks to ensure more precise outputs. Conversely, for generation tasks, we set the temperature to 1, fostering a more diverse range of results and exploring potential worst-case scenarios.</p> <p>The temperature setting of each dataset file is as follows:</p> <pre><code>file_config = {\n        \"disparagement.json\": 1.0,\n        \"preference_force.json\": 1.0,\n        \"preference_plain.json\": 1.0,\n        \"stereotype_agreement.json\": 1.0,\n        \"stereotype_stereoset_classification.json\": 0.0,\n        \"stereotype_query_test.json\":1.0,\n        \"stereotype_recognition.json\":0.0,\n        \"external.json\": 0.0,\n        \"hallucination.json\": 0.0,\n        \"golden_advfactuality.json\": 1.0,\n        \"internal.json\": 1.0,\n        \"sycophancy.json\": 1.0,\n        \"ood_detection.json\":1.0,\n        \"ood_generalization.json\":0.0,\n        \"AdvGLUE.json\":0.0,\n        \"AdvInstruction.json\":1.0,\n        \"jailbreak.json\":1.0,\n        \"exaggerated_safety.json\": 1.0,\n        \"misuse.json\":1.0,\n        \"privacy_awareness_confAIde.json\":0.0,\n        \"privacy_awareness_query.json\": 1.0,\n        \"privacy_leakage.json\": 1.0,\n        \"emotional_awareness.json\": 0.0,\n        \"implicit_ETHICS.json\": 0.0,\n        \"implicit_SocialChemistry101.json\": 0.0\n    }\n</code></pre>"},{"location":"guides/generation_details.html#dataset-task","title":"Dataset &amp; Task","text":"<p>Dataset overview:</p> Dataset Description Num. Exist? Section SQuAD2.0 It combines questions in SQuAD1.1 with over 50,000 unanswerable questions. 100 \u2713 Misinformation CODAH It contains 28,000 commonsense questions. 100 \u2713 Misinformation HotpotQA It contains 113k Wikipedia-based question-answer pairs for complex multi-hop reasoning. 100 \u2713 Misinformation AdversarialQA It contains 30,000 adversarial reading comprehension question-answer pairs. 100 \u2713 Misinformation Climate-FEVER It contains 7,675 climate change-related claims manually curated by human fact-checkers. 100 \u2713 Misinformation SciFact It contains 1,400 expert-written scientific claims pairs with evidence abstracts. 100 \u2713 Misinformation COVID-Fact It contains 4,086 real-world COVID claims. 100 \u2713 Misinformation HealthVer It contains 14,330 health-related claims against scientific articles. 100 \u2713 Misinformation TruthfulQA The multiple-choice questions to evaluate whether a language model is truthful in generating answers to questions. 352 \u2713 Hallucination HaluEval It contains 35,000 generated and human-annotated hallucinated samples. 300 \u2713 Hallucination LM-exp-sycophancy A dataset consists of human questions with one sycophancy response example and one non-sycophancy response example. 179 \u2713 Sycophancy Opinion pairs It contains 120 pairs of opposite opinions. 240 \u2717 Sycophancy WinoBias It contains 3,160 sentences, split for development and testing, created by researchers familiar with the project. 734 \u2713 Stereotype StereoSet It contains the sentences that measure model preferences across gender, race, religion, and profession. 734 \u2713 Stereotype Adult The dataset, containing attributes like sex, race, age, education, work hours, and work type, is utilized to predict salary levels for individuals. 810 \u2713 Disparagement Jailbraek Trigger The dataset contains the prompts based on 13 jailbreak attacks. 1300 \u2717 Jailbreak, Toxicity Misuse (additional) This dataset contains prompts crafted to assess how LLMs react when confronted by attackers or malicious users seeking to exploit the model for harmful purposes. 261 \u2717 Misuse Do-Not-Answer It is curated and filtered to consist only of prompts to which responsible LLMs do not answer. 344 + 95 \u2713 Misuse, Stereotype AdvGLUE A multi-task dataset with different adversarial attacks. 912 \u2713 Natural Noise AdvInstruction 600 instructions generated by 11 perturbation methods. 1200 \u2717 Natural Noise ToolE A dataset with the users' queries which may trigger LLMs to use external tools. 241 \u2713 Out of Domain (OOD) Flipkart A product review dataset, collected starting from December 2022. 400 \u2713 Out of Domain (OOD) DDXPlus A 2022 medical diagnosis dataset comprising synthetic data representing about 1.3 million patient cases. 100 \u2713 Out of Domain (OOD) ETHICS It contains numerous morally relevant scenarios descriptions and their moral correctness. 500 \u2713 Implicit Ethics Social Chemistry 101 It contains various social norms, each consisting of an action and its label. 500 \u2713 Implicit Ethics MoralChoice It consists of different contexts with morally correct and wrong actions. 668 \u2713 Explicit Ethics ConfAIde It contains the description of how information is used. 196 \u2713 Privacy Awareness Privacy Awareness It includes different privacy information queries about various scenarios. 280 \u2717 Privacy Awareness Enron Email It contains approximately 500,000 emails generated by employees of the Enron Corporation. 400 \u2713 Privacy Leakage Xstest It's a test suite for identifying exaggerated safety behaviors in LLMs. 200 \u2713 Exaggerated Safety <p>Task overview:</p> Task Name Metrics Type Eval Section Closed-book QA Accuracy (\u2191) Generation \u25cb Misinformation(Internal) Fact-Checking Macro F-1 (\u2191) Classification \u25cf Misinformation(External) Multiple Choice QA Accuracy (\u2191) Classification \u25cf Hallucination Hallucination Classification Accuracy (\u2191) Classification \u25cf Hallucination Persona Sycophancy Embedding similarity (\u2191) Generation \u25d0 Sycophancy Opinion Sycophancy Percentage change (\u2193) Generation \u25cb Sycophancy Factuality Correction Percentage change (\u2191) Generation \u25cb Adversarial Factuality Jailbreak Attack Evaluation RtA (\u2191) Generation \u25cb Jailbreak Toxicity Measurement Toxicity Value (\u2193) Generation \u25cf Toxicity Misuse Evaluation RtA (\u2191) Generation \u25cb Misuse Exaggerated Safety Evaluation RtA (\u2193) Generation \u25cb Exaggerated Safety Agreement on Stereotypes Accuracy (\u2191) Generation \u25d0 Stereotype Recognition of Stereotypes Agreement Percentage (\u2193) Classification \u25d0 Stereotype Stereotype Query Test RtA (\u2191) Generation \u25cb Stereotype Preference Selection RtA (\u2191) Generation \u25cb Preference Salary Prediction p-value (\u2191) Generation \u25cf Disparagement Adversarial Perturbation in Downstream Tasks ASR (\u2193), RS (\u2191) Generation \u25d0 Natural Noise Adversarial Perturbation in Open-Ended Tasks Embedding similarity (\u2191) Generation \u25d0 Natural Noise OOD Detection RtA (\u2191) Generation \u25cb Out of Domain (OOD) OOD Generalization Micro F1 (\u2191) Classification \u25cb Out of Domain (OOD) Agreement on Privacy Information Pearson\u2019s correlation (\u2191) Classification \u25cf Privacy Awareness Privacy Scenario Test RtA (\u2191) Generation \u25cb Privacy Awareness Probing Privacy Information Usage RtA (\u2191), Accuracy (\u2193) Generation \u25d0 Privacy Leakage Moral Action Judgement Accuracy (\u2191) Classification \u25d0 Implicit Ethics Moral Reaction Selection (Low-Ambiguity) Accuracy (\u2191) Classification \u25d0 Explicit Ethics Moral Reaction Selection (High-Ambiguity) RtA (\u2191) Generation \u25cb Explicit Ethics Emotion Classification Accuracy (\u2191) Classification \u25cf Emotional Awareness"}]}