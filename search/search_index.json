{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"TrustLLM: Trustworthiness in Large Language Models","text":""},{"location":"index.html#about","title":"About","text":"<p>TrustLLM is a comprehensive study of trustworthiness in large language models (LLMs), including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. The document explains how to use the trustllm python package to help you assess the performance of your LLM in trustworthiness more quickly. For more details about TrustLLM, please refer to this link.</p> <p></p>"},{"location":"index.html#before-evaluation","title":"Before Evaluation","text":""},{"location":"index.html#installation","title":"Installation","text":"<p>Create a new environment:</p> <pre><code>conda create --name trustllm python=3.9\n</code></pre> <p>Installation via Github (recommended):</p> <pre><code>git clone git@github.com:HowieHwong/TrustLLM.git\ncd TrustLLM/trustllm_pkg\npip install .\n</code></pre> <p>Installation via <code>pip</code> (deprecated):</p> <pre><code>pip install trustllm\n</code></pre> <p>Installation via <code>conda</code> (deprecated):</p> <pre><code>conda install -c conda-forge trustllm\n</code></pre>"},{"location":"index.html#dataset-download","title":"Dataset Download","text":"<ol> <li>Download TrustLLM dataset from Github:</li> </ol> <pre><code>from trustllm.dataset_download import download_dataset\n\ndownload_dataset(save_path='save_path')\n</code></pre> <ol> <li>Download TrustLLM dataset from Hugginface.</li> </ol>"},{"location":"index.html#generation","title":"Generation","text":"<p>Note</p> <p>Please note that the LLM you use for evaluation should have a certain level of utility. If its generation/NLP capabilities are weak, it may bias the evaluation results (for example, many evaluation samples may be considered invalid).</p> <p>We have added generation section from version 0.2.0. Start your generation from this page.</p>"},{"location":"index.html#start-your-evaluation","title":"Start Your Evaluation","text":"<p>See this page for more details.</p>"},{"location":"index.html#dataset-task","title":"Dataset &amp; Task","text":"<p>Dataset overview</p> Dataset Description Num. Exist? Section SQuAD2.0 It combines questions in SQuAD1.1 with over 50,000 unanswerable questions. 100 \u2713 Misinformation CODAH It contains 28,000 commonsense questions. 100 \u2713 Misinformation HotpotQA It contains 113k Wikipedia-based question-answer pairs for complex multi-hop reasoning. 100 \u2713 Misinformation AdversarialQA It contains 30,000 adversarial reading comprehension question-answer pairs. 100 \u2713 Misinformation Climate-FEVER It contains 7,675 climate change-related claims manually curated by human fact-checkers. 100 \u2713 Misinformation SciFact It contains 1,400 expert-written scientific claims pairs with evidence abstracts. 100 \u2713 Misinformation COVID-Fact It contains 4,086 real-world COVID claims. 100 \u2713 Misinformation HealthVer It contains 14,330 health-related claims against scientific articles. 100 \u2713 Misinformation TruthfulQA The multiple-choice questions to evaluate whether a language model is truthful in generating answers to questions. 352 \u2713 Hallucination HaluEval It contains 35,000 generated and human-annotated hallucinated samples. 300 \u2713 Hallucination LM-exp-sycophancy A dataset consists of human questions with one sycophancy response example and one non-sycophancy response example. 179 \u2713 Sycophancy Opinion pairs It contains 120 pairs of opposite opinions. 240 \u2717 Sycophancy WinoBias It contains 3,160 sentences, split for development and testing, created by researchers familiar with the project. 734 \u2713 Stereotype StereoSet It contains the sentences that measure model preferences across gender, race, religion, and profession. 734 \u2713 Stereotype Adult The dataset, containing attributes like sex, race, age, education, work hours, and work type, is utilized to predict salary levels for individuals. 810 \u2713 Disparagement Jailbraek Trigger The dataset contains the prompts based on 13 jailbreak attacks. 1300 \u2717 Jailbreak, Toxicity Misuse (additional) This dataset contains prompts crafted to assess how LLMs react when confronted by attackers or malicious users seeking to exploit the model for harmful purposes. 261 \u2717 Misuse Do-Not-Answer It is curated and filtered to consist only of prompts to which responsible LLMs do not answer. 344 + 95 \u2713 Misuse, Stereotype AdvGLUE A multi-task dataset with different adversarial attacks. 912 \u2713 Natural Noise AdvInstruction 600 instructions generated by 11 perturbation methods. 1200 \u2717 Natural Noise ToolE A dataset with the users' queries which may trigger LLMs to use external tools. 241 \u2713 Out of Domain (OOD) Flipkart A product review dataset, collected starting from December 2022. 400 \u2713 Out of Domain (OOD) DDXPlus A 2022 medical diagnosis dataset comprising synthetic data representing about 1.3 million patient cases. 100 \u2713 Out of Domain (OOD) ETHICS It contains numerous morally relevant scenarios descriptions and their moral correctness. 500 \u2713 Implicit Ethics Social Chemistry 101 It contains various social norms, each consisting of an action and its label. 500 \u2713 Implicit Ethics MoralChoice It consists of different contexts with morally correct and wrong actions. 668 \u2713 Explicit Ethics ConfAIde It contains the description of how information is used. 196 \u2713 Privacy Awareness Privacy Awareness It includes different privacy information queries about various scenarios. 280 \u2717 Privacy Awareness Enron Email It contains approximately 500,000 emails generated by employees of the Enron Corporation. 400 \u2713 Privacy Leakage Xstest It's a test suite for identifying exaggerated safety behaviors in LLMs. 200 \u2713 Exaggerated Safety <p>Task overview</p> Task Name Metrics Type Eval Section Closed-book QA Accuracy (\u2191) Generation \u25cb Misinformation(Internal) Fact-Checking Macro F-1 (\u2191) Classification \u25cf Misinformation(External) Multiple Choice QA Accuracy (\u2191) Classification \u25cf Hallucination Hallucination Classification Accuracy (\u2191) Classification \u25cf Hallucination Persona Sycophancy Embedding similarity (\u2191) Generation \u25d0 Sycophancy Opinion Sycophancy Percentage change (\u2193) Generation \u25cb Sycophancy Factuality Correction Percentage change (\u2191) Generation \u25cb Adversarial Factuality Jailbreak Attack Evaluation RtA (\u2191) Generation \u25cb Jailbreak Toxicity Measurement Toxicity Value (\u2193) Generation \u25cf Toxicity Misuse Evaluation RtA (\u2191) Generation \u25cb Misuse Exaggerated Safety Evaluation RtA (\u2193) Generation \u25cb Exaggerated Safety Agreement on Stereotypes Accuracy (\u2191) Generation \u25d0 Stereotype Recognition of Stereotypes Agreement Percentage (\u2193) Classification \u25d0 Stereotype Stereotype Query Test RtA (\u2191) Generation \u25cb Stereotype Preference Selection RtA (\u2191) Generation \u25cb Preference Salary Prediction p-value (\u2191) Generation \u25cf Disparagement Adversarial Perturbation in Downstream Tasks ASR (\u2193), RS (\u2191) Generation \u25d0 Natural Noise Adversarial Perturbation in Open-Ended Tasks Embedding similarity (\u2191) Generation \u25d0 Natural Noise OOD Detection RtA (\u2191) Generation \u25cb Out of Domain (OOD) OOD Generalization Micro F1 (\u2191) Classification \u25cb Out of Domain (OOD) Agreement on Privacy Information Pearson\u2019s correlation (\u2191) Classification \u25cf Privacy Awareness Privacy Scenario Test RtA (\u2191) Generation \u25cb Privacy Awareness Probing Privacy Information Usage RtA (\u2191), Accuracy (\u2193) Generation \u25d0 Privacy Leakage Moral Action Judgement Accuracy (\u2191) Classification \u25d0 Implicit Ethics Moral Reaction Selection (Low-Ambiguity) Accuracy (\u2191) Classification \u25d0 Explicit Ethics Moral Reaction Selection (High-Ambiguity) RtA (\u2191) Generation \u25cb Explicit Ethics Emotion Classification Accuracy (\u2191) Classification \u25cf Emotional Awareness"},{"location":"index.html#leaderboard","title":"Leaderboard","text":"<p>If you want to view the performance of all models or upload the performance of your LLM, please refer to this link.</p>"},{"location":"index.html#citation","title":"Citation","text":"<pre><code>@misc{sun2024trustllm,\n      title={TrustLLM: Trustworthiness in Large Language Models}, \n      author={Lichao Sun and Yue Huang and Haoran Wang and Siyuan Wu and Qihui Zhang and Chujie Gao and Yixin Huang and Wenhan Lyu and Yixuan Zhang and Xiner Li and Zhengliang Liu and Yixin Liu and Yijue Wang and Zhikun Zhang and Bhavya Kailkhura and Caiming Xiong and Chaowei Xiao and Chunyuan Li and Eric Xing and Furong Huang and Hao Liu and Heng Ji and Hongyi Wang and Huan Zhang and Huaxiu Yao and Manolis Kellis and Marinka Zitnik and Meng Jiang and Mohit Bansal and James Zou and Jian Pei and Jian Liu and Jianfeng Gao and Jiawei Han and Jieyu Zhao and Jiliang Tang and Jindong Wang and John Mitchell and Kai Shu and Kaidi Xu and Kai-Wei Chang and Lifang He and Lifu Huang and Michael Backes and Neil Zhenqiang Gong and Philip S. Yu and Pin-Yu Chen and Quanquan Gu and Ran Xu and Rex Ying and Shuiwang Ji and Suman Jana and Tianlong Chen and Tianming Liu and Tianyi Zhou and Willian Wang and Xiang Li and Xiangliang Zhang and Xiao Wang and Xing Xie and Xun Chen and Xuyu Wang and Yan Liu and Yanfang Ye and Yinzhi Cao and Yong Chen and Yue Zhao},\n      year={2024},\n      eprint={2401.05561},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"changelog.html","title":"Changelog","text":""},{"location":"changelog.html#todo-in-coming-versions","title":"\u23f0 TODO in Coming Versions","text":"<ul> <li>[x] Faster and simpler evaluation pipeline</li> <li>[ ] Dynamic dataset</li> <li>[ ] More fine-grained datasets</li> <li>[ ] Chinese output evaluation</li> <li>[ ] Downstream application evaluation</li> </ul>"},{"location":"changelog.html#version-030","title":"Version 0.3.0","text":"<p>Release Date: 23rd Apr, 2024</p> <ul> <li>Support parallel retrieval of embeddings when evaluating AdvlInstruction</li> <li>Add exception handling for partial evaluations</li> <li>Fixed some bugs</li> <li>Add evaluation results for ChatGLM3, GLM-4, Mixtral, Llama3-8b, and Llama3-70b (check out)</li> </ul>"},{"location":"changelog.html#version-023-024","title":"Version 0.2.3 &amp; 0.2.4","text":"<p>Release Date: March 2024</p> <ul> <li>Fixed some bugs</li> <li>Support Gemini API</li> </ul>"},{"location":"changelog.html#version-022","title":"Version 0.2.2","text":"<p>Release Date: 1st Feb, 2024</p> <ul> <li>Support awareness evaluation in our new work</li> <li>Support Zhipu API evaluation (GLM-4 &amp; GLM-3-turbo)</li> </ul>"},{"location":"changelog.html#version-021","title":"Version 0.2.1","text":"<p>Release Date: 26th Jan, 2024</p> <ul> <li>Support LLMs in replicate and deepinfra</li> <li>Support easy pipeline for evaluation</li> <li>Support Azure OpenAI API</li> </ul>"},{"location":"changelog.html#version-020","title":"Version 0.2.0","text":"<p>Release Date: 20th Jan, 2024</p> <ul> <li>Add generation section (details)</li> <li>Support concurrency when using auto-evaluation</li> </ul>"},{"location":"changelog.html#version-010","title":"Version 0.1.0","text":"<p>Release Date: 10th Jan, 2024</p> <p>We have released the first version of the TrustLLM assessment tool, which includes all the evaluation methods from our initial research paper.</p>"},{"location":"faq.html","title":"FAQ","text":""},{"location":"faq.html#language-bias","title":"Language Bias","text":"<p>As we mentioned in our paper, the language output of the model will to some extent affect our evaluation results. For the longformer model, its performance in Chinese is relatively poor. Therefore, when calculating RtA, we consider responses with a Chinese character ratio greater than \\(\\alpha\\) as invalid sample (the default setting of \\(\\alpha\\) is 0.3)</p>"},{"location":"guides/evaluation.html","title":"Evaluation","text":""},{"location":"guides/evaluation.html#start-your-evaluation","title":"Start Your Evaluation","text":""},{"location":"guides/evaluation.html#api-setting","title":"API Setting","text":"<p>Before starting the evaluation, you need to first set up your OpenAI API (GPT-4-turbo) and Perspective API (used for measuring toxicity).</p> <pre><code>from trustllm import config\n\nconfig.openai_key = 'your-openai-api-key'\n\nconfig.perspective_key = 'your-perspective-api-key'\n</code></pre> <p>If you're using OpenAI API through Azure, you should set up your Azure api:</p> <pre><code>config.azure_openai = True\n\nconfig.azure_engine = \"your-azure-engine-name\"\n\nconfig.azure_api_base = \"your-azure-api-url (openai.base_url)\"\n</code></pre>"},{"location":"guides/evaluation.html#easy-pipeline","title":"Easy Pipeline","text":"<p>From Version 0.2.1, trustllm toolkit supports easy pipeline for evaluation.</p> <p>We have provided pipelines for all six sections: <code>run_truthfulness</code>, <code>run_safety</code>, <code>run_fairness</code>, <code>run_robustness</code>, <code>run_privacy</code>, <code>run_ethics</code>.</p>"},{"location":"guides/evaluation.html#truthfulness-evaluation","title":"Truthfulness Evaluation","text":"<p>For truthfulness assessment, the <code>run_truthfulness</code> function is used. Provide JSON file paths for internal consistency, external consistency, hallucination scenarios, sycophancy evaluation, and adversarial factuality.  </p> <pre><code>truthfulness_results = run_truthfulness(  \n    internal_path=\"path_to_internal_consistency_data.json\",  \n    external_path=\"path_to_external_consistency_data.json\",  \n    hallucination_path=\"path_to_hallucination_data.json\",  \n    sycophancy_path=\"path_to_sycophancy_data.json\",\n    advfact_path=\"path_to_advfact_data.json\"\n)\n</code></pre> <p>The function will return a dictionary containing results for internal consistency, external consistency, hallucinations, sycophancy (with persona and preference evaluations), and adversarial factuality.     </p>"},{"location":"guides/evaluation.html#safety-evaluation","title":"Safety Evaluation","text":"<p>To assess the safety of your language model, use the <code>run_safety</code> function. You can provide paths to data for jailbreak scenarios, exaggerated safety situations, and misuse potential. Optionally, you can also evaluate for toxicity.  </p> <pre><code>safety_results = run_safety(  \n    jailbreak_path=\"path_to_jailbreak_data.json\",  \n    exaggerated_safety_path=\"path_to_exaggerated_safety_data.json\",  \n    misuse_path=\"path_to_misuse_data.json\",  \n    toxicity_eval=True,  \n    toxicity_path=\"path_to_toxicity_data.json\",  \n    jailbreak_eval_type=\"total\"  \n)  \n</code></pre> <p>The returned dictionary includes results for jailbreak, exaggerated safety, misuse, and toxicity evaluations.  </p>"},{"location":"guides/evaluation.html#fairness-evaluation","title":"Fairness Evaluation","text":"<p>To evaluate the fairness of your language model, use the <code>run_fairness</code> function. This function takes paths to JSON files containing data on stereotype recognition, stereotype agreement, stereotype queries, disparagement, and preference biases.     </p> <pre><code>fairness_results = run_fairness(\n    stereotype_recognition_path=\"path_to_stereotype_recognition_data.json\",      \n    stereotype_agreement_path=\"path_to_stereotype_agreement_data.json\",      \n    stereotype_query_test_path=\"path_to_stereotype_query_test_data.json\",      \n    disparagement_path=\"path_to_disparagement_data.json\",      \n    preference_path=\"path_to_preference_data.json\"   \n)  \n</code></pre> <p>The returned dictionary will include results for stereotype recognition, stereotype agreement, stereotype queries, disparagement, and preference bias evaluations.</p>"},{"location":"guides/evaluation.html#robustness-evaluation","title":"Robustness Evaluation","text":"<p>To evaluate the robustness of your language model, use the <code>run_robustness</code> function. This function accepts paths to JSON files for adversarial GLUE data, adversarial instruction data, out-of-distribution (OOD) detection, and OOD generalization.  </p> <pre><code>robustness_results = run_robustness(  \n    advglue_path=\"path_to_advglue_data.json\",  \n    advinstruction_path=\"path_to_advinstruction_data.json\",  \n    ood_detection_path=\"path_to_ood_detection_data.json\",  \n    ood_generalization_path=\"path_to_ood_generalization_data.json\"  \n)  \n</code></pre> <p>The function returns a dictionary with the results of adversarial GLUE, adversarial instruction, OOD detection, and OOD generalization evaluations.  </p>"},{"location":"guides/evaluation.html#privacy-evaluation","title":"Privacy Evaluation","text":"<p>To conduct privacy evaluations, use the <code>run_privacy</code> function. It allows you to specify paths to datasets for privacy conformity, privacy awareness queries, and privacy leakage scenarios.  </p> <pre><code>privacy_results = run_privacy(  \n    privacy_confAIde_path=\"path_to_privacy_confaide_data.json\",  \n    privacy_awareness_query_path=\"path_to_privacy_awareness_query_data.json\",  \n    privacy_leakage_path=\"path_to_privacy_leakage_data.json\"  \n)  \n</code></pre> <p>The function outputs a dictionary with results for privacy conformity AIde, normal and augmented privacy awareness queries, and privacy leakage evaluations.  </p>"},{"location":"guides/evaluation.html#ethics-evaluation","title":"Ethics Evaluation","text":"<p>To evaluate the ethical considerations of your language model, use the <code>run_ethics</code> function. You can specify paths to JSON files containing explicit ethics, implicit ethics, and awareness data.  </p> <pre><code>results = run_ethics(  \n    explicit_ethics_path=\"path_to_explicit_ethics_data.json\",  \n    implicit_ethics_path=\"path_to_implicit_ethics_data.json\",  \n    awareness_path=\"path_to_awareness_data.json\"  \n)  \n</code></pre> <p>The function returns a dictionary containing the results of the explicit ethics evaluation (with low and high levels), implicit ethics evaluation (ETHICS and social norm types), and emotional awareness evaluation.  </p>"},{"location":"guides/evaluation.html#truthfulness","title":"Truthfulness","text":"<p>Four subsections in truthfulness evaluation:</p> <ul> <li>Misinformation: <code>external.json</code>, <code>internal.json</code></li> <li>Hallucination: <code>hallucination.json</code></li> <li>Sycophancy: <code>sycophancy.json</code></li> <li>Adversarial Factuality: <code>golden_advfactuality.json</code></li> </ul> <p>Requirement:</p> <p></p> <ul> <li>openai api (gpt-4-turbo)</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm import truthfulness\nfrom trustllm import file_process\nfrom trustllm import config\n\nevaluator = truthfulness.TruthfulnessEval()\n</code></pre> <p>Misinformation evaluation:</p> <pre><code>misinformation_internal_data = file_process.load_json('misinformation_internal_data_json_path')\nprint(evaluator.internal_eval(misinformation_internal_data))\n\nmisinformation_external_data = file_process.load_json('misinformation_external_data_json_path')\nprint(evaluator.external_eval(misinformation_external_data))\n</code></pre> <p>Hallucination evaluation:</p> <pre><code>hallucination_data = file_process.load_json('hallucination_data_json_path')\nprint(evaluator.hallucination_eval(hallucination_data))\n</code></pre> <p>Sycophancy evaluation (<code>eval_type</code>: type of evaluation, either <code>persona</code> or <code>preference</code>): <pre><code>sycophancy_data = file_process.load_json('sycophancy_data_json_path')\nprint(evaluator.sycophancy_eval(sycophancy_data, eval_type='persona'))\nprint(evaluator.sycophancy_eval(sycophancy_data, eval_type='preference'))\n</code></pre></p> <p>Adversarial factuality evaluation: <pre><code>adv_fact_data = file_process.load_json('adv_fact_data_json_path')\nprint(evaluator.advfact_eval(adv_fact_data))\n</code></pre></p>"},{"location":"guides/evaluation.html#safety","title":"Safety","text":"<p>Four subsections in safety evaluation:</p> <ul> <li>Jailbreak: <code>jailbreak.json</code></li> <li>Exaggerated Safety: <code>exaggerated_safety.json</code></li> <li>Toxicity: <code>jailbreak.json</code></li> <li>Misuse: <code>misuse.json</code></li> </ul> <p>Requirement:</p> <p> </p> <ul> <li>openai api (gpt-4-turbo)</li> <li>perspective api</li> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm import safety\nfrom trustllm import file_process\nfrom trustllm import config\n\nevaluator = safety.SafetyEval()\n</code></pre> <p>Jailbreak evaluation (<code>eval_type</code>: type of evaluation, either <code>total</code> or <code>single</code>):</p> <pre><code>jailbreak_data = file_process.load_json('jailbreak_data_json_path')\nprint(evaluator.jailbreak_eval(jailbreak_data, eval_type='total')) # return overall RtA\nprint(evaluator.jailbreak_eval(jailbreak_data, eval_type='single')) # return RtA dict for each kind of jailbreak ways\n</code></pre> <p>Exaggerated evaluation:</p> <pre><code>exaggerated_data = file_process.load_json('exaggerated_data_json_path')\nprint(evaluator.exaggerated_eval(exaggerated_data))\n</code></pre> <p>Toxicity evaluation:</p> <p>Note</p> <p>Toxicity data for evaluation is based on the evaluated results of jailbreak data. You should first evaluate jailbreak data and then use evaluated results to conduct toxicity evaluation.</p> <pre><code># If you have not saved evaluated jailbreak results\njailbreak_data = file_process.load_json('jailbreak_data_json_path')\neval_data, _ = evaluator.jailbreak_eval(jailbreak_data, eval_type='total/single', return_data=True)\nprint(evaluator.toxicity_eval(eval_data))\n\n# If you have saved evaluated jailbreak results\ntoxicity_data = file_process.load_json('evaluated_jailbreak_results_json_path') # load eval data for toxicity evaluation\nprint(evaluator.toxicity_eval(toxicity_data))\n</code></pre> <p>Misuse evaluation:</p> <pre><code>misuse_data = file_process.load_json('misuse_data_json_path')\nprint(evaluator.misuse_eval(misuse_data))\n</code></pre>"},{"location":"guides/evaluation.html#fairness","title":"Fairness","text":"<p>Three subsections in fairness evaluation:</p> <ul> <li>Stereotype: <code>stereotype_agreement.json</code>, <code>stereotype_recognition.json</code>, <code>stereotype_query_test.json</code></li> <li>Disparagement: <code>disparagement.json</code></li> <li>Preference: <code>preference_force.json</code></li> </ul> <p>Requirement:</p> <p> </p> <ul> <li>openai api (gpt-4-turbo)</li> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm import fairness\nfrom trustllm import file_process\nfrom trustllm import config\n\nevaluator = fairness.FairnessEval()\n</code></pre> <p>Stereotype evaluation:</p> <pre><code>stereotype_recognition_data = file_process.load_json('stereotype_recognition_data_json_path')\nprint(evaluator.stereotype_recognition_eval(stereotype_recognition_data))\n\nstereotype_agreement_data = file_process.load_json('stereotype_agreement_data_json_path')\nprint(evaluator.stereotype_agreement_eval(stereotype_agreement_data))\n\nstereotype_query_test_data = file_process.load_json('stereotype_query_test_data_json_path')\nprint(evaluator.stereotype_query_eval(stereotype_query_test_data))\n</code></pre> <p>Disparagement evaluation: <pre><code>disparagement_data = file_process.load_json('disparagement_data_json_path')\nprint(evaluator.disparagement_eval(disparagement_data))\n</code></pre></p> <p>Preference evaluation: <pre><code>preference_data = file_process.load_json('preference_data_json_path')\nprint(evaluator.preference_eval(preference_data))\n</code></pre></p>"},{"location":"guides/evaluation.html#robustness","title":"Robustness","text":"<p>Two subsections in robustness evaluation:</p> <ul> <li>Natural noise: <code>advglue.json</code>, <code>advinstruction.json</code></li> <li>Out of distribution: <code>ood_generalization.json</code>, <code>ood_detection.json</code></li> </ul> <p>Requirement:</p> <p> </p> <ul> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm import robustness\nfrom trustllm import file_process\nfrom trustllm import config\n\nevaluator = robustness.RobustnessEval()\n</code></pre> <p>Natural noise evaluation:</p> <pre><code>advglue_data = file_process.load_json('advglue_data_json_path')\nprint(evaluator.advglue_eval(advglue_data))\n\nadvinstruction_data = file_process.load_json('advinstruction_data_json_path')\nprint(evaluator.advglue_eval(advinstruction_data))\n</code></pre> <p>OOD evaluation:</p> <pre><code>ood_detection_data = file_process.load_json('ood_detection_data_json_path')\nprint(evaluator.ood_detection(ood_detection_data))\n\nood_generalization_data = file_process.load_json('ood_generalization_data_json_path')\nprint(evaluator.ood_generalization(ood_generalization_data))\n</code></pre>"},{"location":"guides/evaluation.html#privacy","title":"Privacy","text":"<p>Two subsections in privacy evaluation:</p> <ul> <li>Privacy awareness: <code>privacy_awareness_confAIde.json</code>, <code>privacy_awareness_query.json</code></li> <li>Privacy leakage: <code>privacy_leakage.json</code></li> </ul> <p>Requirement:</p> <p> </p> <ul> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm import privacy\nfrom trustllm import file_process\nfrom trustllm import config\n\nevaluator = privacy.PrivacyEval()\n</code></pre> <p>Privacy awareness:</p> <pre><code>privacy_confAIde_data = file_process.load_json('privacy_confAIde_data_json_path')\nprint(evaluator.ConfAIDe_eval(privacy_confAIde_data))\n\nprivacy_awareness_query_data = file_process.load_json('privacy_awareness_query_data_json_path')\nprint(evaluator.awareness_query_eval(privacy_awareness_query_data, type='normal'))\nprint(evaluator.awareness_query_eval(privacy_awareness_query_data, type='aug'))\n</code></pre> <p>Privacy leakage:</p> <pre><code>privacy_leakage_data = file_process.load_json('privacy_leakage_data_json_path')\nprint(evaluator.leakage_eval(privacy_leakage_data))\n</code></pre>"},{"location":"guides/evaluation.html#machine-ethics","title":"Machine Ethics","text":"<p>Three subsections in machine ethics evaluation:</p> <p>Implicit ethics: <code>implicit_ETHICS.json</code>, <code>implicit_SocialChemistry101.json</code> Explicit ethics: <code>explicit_moralchoice.json</code> Awareness: <code>awareness.json</code> </p> <p>Requirement:</p> <p> </p> <ul> <li>openai api (gpt-4-turbo)</li> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm import ethics\nfrom trustllm import file_process\nfrom trustllm import config\n\nevaluator = ethics.EthicsEval()\n</code></pre> <p>Explicit ethics:</p> <p><pre><code>explicit_ethics_data = file_process.load_json('explicit_ethics_data_json_path')\nprint(evaluator.explicit_ethics_eval(explicit_ethics_data, eval_type='low'))\nprint(evaluator.explicit_ethics_eval(explicit_ethics_data, eval_type='high'))\n</code></pre> Implicit ethics:</p> <pre><code>implicit_ethics_data = file_process.load_json('implicit_ethics_data_json_path')\n# evaluate ETHICS dataset\nprint(evaluator.implicit_ethics_eval(implicit_ethics_data, eval_type='ETHICS'))\n# evaluate social_norm dataset\nprint(evaluator.implicit_ethics_eval(implicit_ethics_data, eval_type='social_norm'))\n</code></pre> <p>Awareness:</p> <pre><code>awareness_data = file_process.load_json('awareness_data_json_path')\nprint(evaluator.awareness_eval(awareness_data))\n</code></pre>"},{"location":"guides/generation_details.html","title":"Generation","text":""},{"location":"guides/generation_details.html#generation-results","title":"Generation Results","text":"<p>The trustllm toolkit currently supports the generation of over a dozen models.  You can use the trustllm toolkit to generate output results for specified models on the trustllm benchmark.</p>"},{"location":"guides/generation_details.html#supported-llms","title":"Supported LLMs","text":"<ul> <li><code>Baichuan-13b</code></li> <li><code>Baichuan2-13b</code></li> <li><code>Yi-34b</code></li> <li><code>ChatGLM2 - 6B</code></li> <li><code>ChatGLM3 -6B</code></li> <li><code>Vicuna-13b</code></li> <li><code>Vicuna-7b</code></li> <li><code>Vicuna-33b</code></li> <li><code>Llama2-7b</code></li> <li><code>Llama2-13b</code></li> <li><code>Llama2-70b</code></li> <li><code>Koala-13b</code></li> <li><code>Oasst-12b</code></li> <li><code>Wizardlm-13b</code></li> <li><code>Mixtral-8x7B</code></li> <li><code>Mistral-7b</code></li> <li><code>Dolly-12b</code></li> <li><code>bison-001-text</code></li> <li><code>ERNIE</code></li> <li><code>ChatGPT (gpt-3.5-turbo)</code></li> <li><code>GPT-4</code></li> <li><code>Claude-2</code></li> <li><code>Gemini-pro</code></li> <li>other LLMs in huggingface</li> </ul>"},{"location":"guides/generation_details.html#start-your-generation","title":"Start Your Generation","text":"<p>The <code>LLMGeneration</code> class is designed for result generation, supporting the use of both local and online models. It is used for evaluating the performance of models in different tasks such as ethics, privacy, fairness, truthfulness, robustness, and safety.</p> <p>Dataset</p> <p>You should firstly download TrustLLM dataset (details) and the downloaded dataset dict has the following structure:</p> <pre><code>|-TrustLLM\n    |-Safety\n        |-Json_File_A\n        |-Json_File_B\n        ...\n    |-Truthfulness\n        |-Json_File_A\n        |-Json_File_B\n        ...\n    ...\n</code></pre> <p>API setting:</p> <p>If you need to evaluate an API LLM, please set the following API according to your requirements.</p> <pre><code>from trustllm import config\n\nconfig.deepinfra_api = \"deepinfra api\"\n\nconfig.claude_api = \"claude api\"\n\nconfig.openai_key = \"openai api\"\n\nconfig.palm_api = \"palm api\"\n\nconfig.ernie_client_id = \"ernie client id\"\n\nconfig.ernie_client_secret = \"ernie client secret\"\n\nconfig.ernie_api = \"ernie api\"\n</code></pre> <p>Generation template:</p> <pre><code>from trustllm.generation.generation import LLMGeneration\n\nllm_gen = LLMGeneration(\n    model_path=\"your model name\", \n    test_type=\"test section\", \n    data_path=\"your dataset file path\",\n    model_name=\"\", \n    online_model=False, \n    use_deepinfra=False,\n    use_replicate=False,\n    repetition_penalty=1.0,\n    num_gpus=1, \n    max_new_tokens=512, \n    debug=False\n)\n\nllm_gen.generation_results()\n</code></pre> <p>Args:</p> <ul> <li> <p><code>model_path</code> (<code>Required</code>, <code>str</code>): Path to the local model. LLM list:</p> </li> <li> <p>If you're using locally public model (huggingface) or use deepinfra online models:   <pre><code>'baichuan-inc/Baichuan-13B-Chat', \n'baichuan-inc/Baichuan2-13B-chat', \n'01-ai/Yi-34B-Chat', \n'THUDM/chatglm2-6b', \n'THUDM/chatglm3-6b', \n'lmsys/vicuna-13b-v1.3', \n'lmsys/vicuna-7b-v1.3', \n'lmsys/vicuna-33b-v1.3', \n'meta-llama/Llama-2-7b-chat-hf', \n'meta-llama/Llama-2-13b-chat-hf', \n'TheBloke/koala-13B-HF', \n'OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5', \n'WizardLM/WizardLM-13B-V1.2', \n'mistralai/Mixtral-8x7B-Instruct-v0.1', \n'meta-llama/Llama-2-70b-chat-hf', \n'mistralai/Mistral-7B-Instruct-v0.1', \n'databricks/dolly-v2-12b', \n'bison-001', \n'ernie', \n'chatgpt', \n'gpt-4', \n'claude-2'\n... (other LLMs in huggingface)\n</code></pre></p> </li> <li> <p>If you're using use online models in replicate, You can find model_path in this link:   <pre><code>'meta/llama-2-70b-chat',\n'meta/llama-2-13b-chat',\n'meta/llama-2-7b-chat',\n'mistralai/mistral-7b-instruct-v0.1',\n'replicate/vicuna-13b',\n... (other LLMs in replicate)\n</code></pre></p> </li> <li> <p><code>test_type</code> (<code>Required</code>, <code>str</code>): Type of evaluation task, including <code>'robustness'</code>, <code>'truthfulness'</code>, <code>'fairness'</code>, <code>'ethics'</code>, <code>'safety'</code>, <code>'privacy'</code>.</p> </li> <li><code>data_path</code> (<code>Required</code>, <code>str</code>): Path to the root dataset, default is 'TrustLLM'.</li> <li><code>online_model</code> (<code>Optional</code>, <code>bool</code>): Whether to use an online model, default is False.</li> <li><code>use_deepinfra</code> (<code>Optional</code>, <code>bool</code>): Whether to use an online model in <code>deepinfra</code>, default is False. (Only work when <code>oneline_model=True</code>)</li> <li><code>usr_replicate</code> (<code>Optional</code>, <code>bool</code>): Whether to use an online model in <code>replicate</code>, default is False. (Only work when <code>oneline_model=True</code>)</li> <li><code>repetition_penalty</code> (<code>Optional</code>, <code>float</code>): Repetition penalty setting, default is 1.0.</li> <li><code>num_gpus</code> (<code>Optional</code>, <code>int</code>): Number of GPUs to use, default is 1.</li> <li><code>max_new_tokens</code> (<code>Optional</code>, <code>int</code>): Maximum number of new tokens in generated text, default is 512.</li> <li><code>device</code> (<code>Optional</code>, <code>str</code>): Specify the device to use, default is 'cuda:0'.</li> </ul> <p>Here is a usage example:</p> <pre><code>from trustllm.generation.generation import LLMGeneration\n\nllm_gen = LLMGeneration(\n    model_path=\"meta-llama/Llama-2-7b-chat-hf\", \n    test_type=\"safety\", \n    data_path=\"TrustLLM\"\n)\n\nllm_gen.generation_results()\n</code></pre>"},{"location":"guides/generation_details.html#dont-have-enough-computing-resource","title":"Don't have enough computing resource?","text":"<p>If you don't have sufficient computing resources to run HuggingFace models locally, we recommend using online models.  We provide an online model interface through deepinfra or replicate.</p> <p>Currently supported online models in deepinfra include:</p> <ul> <li><code>llama2-70b</code></li> <li><code>mistral-7b</code></li> <li><code>dolly-12b</code></li> <li><code>llama2-13b</code></li> <li><code>mixtral-8x7B</code></li> <li><code>yi-34b</code></li> </ul> <p>Currently supported online models in replicate can be found in this link.</p> <p>Before using online model to generate your results, you should set your <code>deepinfra</code> API or <code>replicate</code> API:</p> <pre><code>from trustllm import config\n\nconfig.deepinfra_api = \"deepinfra api\"\n\nconfig.replicate_api = \"replicate api\"\n</code></pre> <p>Then, you should set <code>oneline_model=True</code>:</p> <pre><code>from trustllm.generation.generation import LLMGeneration\n\nllm_gen = LLMGeneration(\n    model_path=\"meta-llama/Llama-2-7b-chat-hf\", \n    test_type=\"safety\", \n    data_path=\"TrustLLM\",\n    online_model=True\n)\n\nllm_gen.generation_results()\n</code></pre>"},{"location":"guides/generation_details.html#temperature-setting","title":"Temperature Setting","text":"<p>We categorize the tasks in the benchmark into two main groups: Generation and Classification. </p> <p>We employ a temperature setting of 0 for classification tasks to ensure more precise outputs. Conversely, for generation tasks, we set the temperature to 1, fostering a more diverse range of results and exploring potential worst-case scenarios.</p> <p>The temperature setting of each dataset file is as follows:</p> <pre><code>file_config = {\n        \"disparagement.json\": 1.0,\n        \"preference_force.json\": 1.0,\n        \"preference_plain.json\": 1.0,\n        \"stereotype_agreement.json\": 1.0,\n        \"stereotype_stereoset_classification.json\": 0.0,\n        \"stereotype_query_test.json\":1.0,\n        \"stereotype_recognition.json\":0.0,\n        \"external.json\": 0.0,\n        \"hallucination.json\": 0.0,\n        \"golden_advfactuality.json\": 1.0,\n        \"internal.json\": 1.0,\n        \"sycophancy.json\": 1.0,\n        \"ood_detection.json\":1.0,\n        \"ood_generalization.json\":0.0,\n        \"AdvGLUE.json\":0.0,\n        \"AdvInstruction.json\":1.0,\n        \"jailbreak.json\":1.0,\n        \"exaggerated_safety.json\": 1.0,\n        \"misuse.json\":1.0,\n        \"privacy_awareness_confAIde.json\":0.0,\n        \"privacy_awareness_query.json\": 1.0,\n        \"privacy_leakage.json\": 1.0,\n        \"awareness.json\": 0.0,\n        \"implicit_ETHICS.json\": 0.0,\n        \"implicit_SocialChemistry101.json\": 0.0\n    }\n</code></pre>"}]}